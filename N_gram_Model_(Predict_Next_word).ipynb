{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3qMutgdW1ZO",
        "outputId": "717b38bd-3053-45b6-f10f-d36f5a05e81f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/indian.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict\n",
        "nltk.download('punkt')\n",
        "nltk.download('indian')\n",
        "\n",
        "hindi_sentences = [\n",
        "    \"मेरा नाम है।\",\n",
        "    \"मेरा खाना तैयार है।\",\n",
        "    \"मेरा दोस्त आ रहा है।\",\n",
        "    \"तुम कैसे हो?\",\n",
        "    \"वह आया।\",\n",
        "    \"हम जाएँगे।\",\n",
        "    \"मैं समझता हूँ।\",\n",
        "    \"यह बहुत अच्छा है।\",\n",
        "    \"कृपया ध्यान दें।\",\n",
        "    \"कौन हो तुम?\",\n",
        "    \"आपका स्वागत है।\",\n",
        "    \"धन्यवाद।\"\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return sentences, words\n",
        "\n",
        "def generate_trigrams(input_list):\n",
        "    return list(ngrams(input_list, 3))\n",
        "\n",
        "def predict_next_words(input_phrase, hindi_sentences):\n",
        "    input_words = nltk.word_tokenize(input_phrase)\n",
        "    hindi_corpus_words = []\n",
        "    for sentence in hindi_sentences:\n",
        "        _, words = tokenize_text(sentence)\n",
        "        hindi_corpus_words.extend(words)\n",
        "    trigrams = generate_trigrams(hindi_corpus_words)\n",
        "    next_words_count = defaultdict(int)\n",
        "    for idx, word in enumerate(trigrams):\n",
        "        if word[0] == input_words[-2] and word[1] == input_words[-1]:\n",
        "            next_word = trigrams[idx][2]\n",
        "            next_words_count[next_word] += 1\n",
        "    total_count = sum(next_words_count.values())\n",
        "    next_words_probs = {word: count / total_count for word, count in next_words_count.items()}\n",
        "    return next_words_probs\n",
        "\n",
        "input_phrase = \"मेरा नाम\"\n",
        "predictions = predict_next_words(input_phrase, hindi_sentences)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "QB4u28g64Op5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b947e2b-3d85-43e5-b3d0-71c3959346fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'है।': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theory:\n",
        "\n",
        "Statistical language models, in its essence, are the type of models that assign probabilities to the sequences of words. In this article, we’ll understand the simplest model that assigns probabilities to sentences and sequences of words, the n-gram\n",
        "You can think of an N-gram as the sequence of N words, by that notion, a 2-gram (or bigram) is a two-word sequence of words like “please turn”, “turn your”, or ”your homework”, and a 3-gram (or trigram) is a three-word sequence of words like “please turn your”, or “turn your homework”\n",
        "\n",
        "\\\n",
        "\\\n",
        "\n",
        "Algortihm:\n",
        "\n",
        "An N-gram model is a statistical language model that captures the likelihood of a word based on its context, considering the N preceding words. In this case, I'll provide you with a step-by-step algorithm for building an N-gram language model, where N represents the number of preceding words considered for prediction. We'll use Python for the implementation. Let's consider a simple example of a bigram model (N=2) for simplicity.\n",
        "\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "\n",
        "**Bigram (2-gram) Model:**\n",
        "\n",
        "Step 1: **Tokenization**\n",
        "\n",
        "- Tokenize your input text into a list of words or tokens.\n",
        "- Count the number of unique words in the text.\n",
        "\n",
        "\n",
        "Step 2: **Create N-grams**\n",
        "\n",
        "- Generate bigrams (2-grams) by pairing each word with its consecutive word.\n",
        "\n",
        "\n",
        "\n",
        "Step 3: **Build a Frequency Distribution**\n",
        "\n",
        "- Calculate the frequency of each bigram using Python's `nltk.FreqDist`.\n",
        "\n",
        "\n",
        "Step 4: **Calculate Conditional Probabilities**\n",
        "\n",
        "- Calculate conditional probabilities for each bigram, which represent the likelihood of a word given the previous word.\n",
        "\n",
        "\n",
        "\n",
        "Step 5: **Generate Text**\n",
        "\n",
        "- You can generate text using the trained bigram model. Start with an initial word and predict the next word based on the conditional probabilities.\n",
        "\n",
        "\\\n",
        "\\\n",
        "\n",
        "\n",
        "This algorithm builds a simple bigram model and generates text based on the conditional probabilities. For more accurate and sophisticated N-gram models, you can extend this approach to higher N values, such as trigrams (3-grams) or more, and use larger text corpora for training.\n",
        "\n",
        "The step count may vary depending on the specific implementation and complexity of your N-gram model, but these steps provide a general guideline for building and using an N-gram model."
      ],
      "metadata": {
        "id": "Hv4Ul_RT3AEW"
      }
    }
  ]
}