{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BhMqr_kpb7eZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f6a7ca3-deed-4907-c180-4982e17b31c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (NP the/DT book/NN) has/VBZ (NP many/JJ chapters/NNS))\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "sentence = [\n",
        "   (\"the\", \"DT\"),\n",
        "   (\"book\", \"NN\"),\n",
        "   (\"has\",\"VBZ\"),\n",
        "   (\"many\",\"JJ\"),\n",
        "   (\"chapters\",\"NNS\")\n",
        "]\n",
        "chunker = nltk.RegexpParser(\n",
        "   r'''\n",
        "   NP:{<DT><NN.*><.*>*<NN.*>}\n",
        "   }<VB.*>{\n",
        "   '''\n",
        ")\n",
        "chunker.parse(sentence)\n",
        "Output = chunker.parse(sentence)\n",
        "print(Output)\n",
        "# Output.draw()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz5eFUdA7r8w",
        "outputId": "f81373eb-8457-49e3-bb7b-b5a73fe8dfd8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open ('/content/Sample.txt')\n",
        "data_text = file.read()\n",
        "sentences = nltk.sent_tokenize(data_text)\n",
        "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
        "for sent in tagged_sentences:\n",
        "  print (nltk.ne_chunk(sent))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHOerDRNcZ1E",
        "outputId": "7b758202-5cbd-4786-bc33-655c354a8fbd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S I/PRP am/VBP a/DT good/JJ boy/NN ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking is a process in natural language processing (NLP) that involves grouping words or tokens into meaningful chunks, such as noun phrases or verb phrases. It's often used in tasks like information extraction and text analysis. Here's a step-wise algorithm for chunking using the NLTK library in Python, with step counts:\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "\n",
        "**Chunking Algorithm:**\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "\n",
        "1. **Import Necessary Libraries:**\n",
        "  \n",
        "\n",
        "2. **Tokenize the Text:**\n",
        "   Tokenize the input text into words or tokens. You can use NLTK's tokenization functions or libraries like spaCy or the Natural Language Toolkit (NLTK) to accomplish this.\n",
        "\n",
        "3. **Part-of-Speech Tagging:**\n",
        "   Perform part-of-speech (POS) tagging on the tokens to label each word with its POS category (e.g., noun, verb, adjective).\n",
        "\n",
        "  \n",
        "\n",
        "4. **Define Chunking Rules:**\n",
        "   Create chunking rules using regular expressions. Define patterns that match the chunks you want to extract, such as noun phrases.\n",
        "\n",
        "  \n",
        "\n",
        "5. **Create a Chunk Parser:**\n",
        "   Use the defined grammar to create a chunk parser.\n",
        "\n",
        "  \n",
        "\n",
        "6. **Apply Chunking:**\n",
        "   Apply the chunk parser to the tagged tokens to extract chunks based on your defined patterns.\n",
        "\n",
        "   \n",
        "\n",
        "7. **Visualize or Access the Chunks:**\n",
        "   You can visualize the chunks or access them programmatically. For visualization, you can use NLTK's built-in methods to draw the parse tree or simply print the chunks.\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "Dzb4z4KL7xYa"
      }
    }
  ]
}